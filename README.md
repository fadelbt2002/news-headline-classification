# Beyond the Words: News Headline Classification

A machine learning project that classifies news headlines by source (NBC vs Fox News) using advanced NLP models including LSTM, BERT, and GPT-2.

## ğŸ“Š Project Overview

This project investigates how different text preprocessing techniques and model architectures affect the classification of news headlines between Fox News and NBC News. Our research focuses on three key hypotheses:

1. **Data Cleaning Hypothesis**: Standard text preprocessing may hurt performance by removing contextual cues
2. **Model Architecture Hypothesis**: Transformer models (BERT/GPT-2) will outperform LSTM models
3. **Feature Enhancement Hypothesis**: POS tags will improve LSTM performance but not transformer performance

## ğŸ¯ Key Findings

- **Best Model**: BiLSTM with POS tags achieved **95% accuracy** and **0.95 F1 score**
- **Surprising Result**: LSTM outperformed both BERT and GPT-2 (which plateaued around 84-85% F1)
- **Preprocessing Impact**: Aggressive text normalization degrades performance across all models
- **Feature Engineering**: POS tags significantly help LSTM but don't improve transformers

## ğŸ“ˆ Model Performance

| Model | Accuracy | F1 Score | NBC F1 | Fox F1 |
|-------|----------|----------|---------|---------|
| **BiLSTM + POS** | **95%** | **0.95** | **0.95** | **0.95** |
| BERT | 84% | 0.84 | 0.84 | 0.85 |
| GPT-2 | 83% | 0.85 | 0.85 | 0.86 |
| TF-IDF + LR | 68% | 0.67 | 0.63 | 0.71 |

## ğŸ—ï¸ Project Structure

```
news-headline-classification/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original scraped data
â”‚   â”œâ”€â”€ processed/              # Cleaned and preprocessed data
â”‚   â””â”€â”€ urls.csv               # Source URLs for scraping
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_collection/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scraper.py         # Beautiful Soup web scraper
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py    # Text preprocessing utilities
â”‚   â”‚   â””â”€â”€ feature_extractor.py # POS/NER feature extraction
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline.py        # TF-IDF + Logistic Regression
â”‚   â”‚   â”œâ”€â”€ lstm_model.py      # BiLSTM with POS embeddings
â”‚   â”‚   â”œâ”€â”€ bert_model.py      # Fine-tuned BERT classifier
â”‚   â”‚   â””â”€â”€ gpt2_model.py      # GPT-2 with POS features
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py         # Evaluation utilities
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ config.py          # Configuration settings
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_results_analysis.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_models.py        # Training pipeline
â”‚   â””â”€â”€ evaluate_models.py     # Evaluation pipeline
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ model_performance.json
â”‚   â”œâ”€â”€ confusion_matrices/
â”‚   â””â”€â”€ visualizations/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ final_report.pdf       # Detailed research report
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_models.py
    â””â”€â”€ test_scraper.py
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/news-headline-classification.git
cd news-headline-classification

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Data Collection

```bash
# Scrape headlines from provided URLs
python scripts/collect_data.py --urls data/urls.csv --output data/raw/
```

### Training Models

```bash
# Train all models
python scripts/train_models.py

# Train specific model
python scripts/train_models.py --model lstm --use-pos-tags
```

### Evaluation

```bash
# Evaluate all models
python scripts/evaluate_models.py

# Generate visualizations
python scripts/generate_plots.py
```

## ğŸ› ï¸ Usage Examples

### Basic Classification

```python
from src.models.lstm_model import LSTMClassifier
from src.preprocessing.text_cleaner import TextPreprocessor

# Initialize model and preprocessor
model = LSTMClassifier.load('results/best_lstm_model.pkl')
preprocessor = TextPreprocessor(use_pos_tags=True)

# Classify a headline
headline = "Biden announces new climate initiative"
processed = preprocessor.transform([headline])
prediction = model.predict(processed)
print(f"Predicted source: {'NBC' if prediction[0] == 0 else 'Fox News'}")
```

### Model Training

```python
from src.models.bert_model import BERTClassifier
from src.utils.config import Config

# Initialize and train BERT model
config = Config()
model = BERTClassifier(config)
model.train(train_data, val_data, epochs=10)
model.save('results/bert_model.pkl')
```

## ğŸ“Š Data Analysis

Our exploratory data analysis revealed key differences between Fox News and NBC headlines:

- **Length**: Fox News headlines average 12.3 words vs NBC's 11.8 words
- **Vocabulary**: Different emphasis on political figures and institutions
- **POS Patterns**: NBC uses more proper nouns and determiners; Fox uses more adjectives
- **Stylistic Cues**: Punctuation and capitalization patterns differ significantly

## ğŸ”¬ Methodology

### Data Collection
- Web scraping using Beautiful Soup
- Headlines collected from provided URL lists
- 80/20 train/validation split with stratification

### Preprocessing Variants
1. **Raw**: Minimal preprocessing (best for transformers)
2. **Normalized**: Lowercasing, punctuation removal, lemmatization
3. **Enhanced**: Raw text + POS tags (best for LSTM)

### Model Architectures

**LSTM**: BiLSTM with GloVe embeddings and POS tag embeddings
**BERT**: Fine-tuned BERT-base-uncased with custom classification head
**GPT-2**: GPT-2 with custom POS integration layer
**Baseline**: TF-IDF features with Logistic Regression

## ğŸ“ Key Insights

1. **Preprocessing Paradox**: Common NLP preprocessing techniques hurt performance
2. **Context Limitations**: Short headlines limit the advantage of transformer models
3. **Syntactic Signals**: Explicit POS features help simpler models compete with transformers
4. **Style Over Content**: Classification relies more on stylistic differences than topic

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ“š References

1. HaCohen-Kerner, Y., Miller, J., & Yigal, G. (2020). The influence of preprocessing on text classification using a bag-of-words representation. PLOS ONE, 15(5), e0232525.


## ğŸ“§ Contact

For questions about this project, please open an issue or contact the team members.

---
